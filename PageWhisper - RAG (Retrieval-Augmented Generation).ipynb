{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6597fcbf-b38f-4934-b8aa-39b9c8459422",
   "metadata": {},
   "source": [
    "# PageWhisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea589a-eee4-4e9c-8880-d1b542889c07",
   "metadata": {},
   "source": [
    "### A RAG (Retrieval-Augmented Generation) based system that allows users to upload PDFs and ask intelligent questions, generating context-aware answers using AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6249cb-2520-48a4-a135-bb1bb11329b4",
   "metadata": {},
   "source": [
    "#### Importing the required libraries and frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189a59cb-7763-4bfd-95b0-f84048ebced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "from ollama import Client\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b532dbe3-8738-4fcc-a92a-c10952c8c9e2",
   "metadata": {},
   "source": [
    "#### Function to extract text from a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9384e614-4e08-421a-bb5e-a13fa95e5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(pdf_path, status):\n",
    "    status.value = \"Extracting text from PDF...\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    full_text = \"\\n\".join(page.page_content for page in pages)\n",
    "    text_file_path = \"extracted_text.txt\"\n",
    "    with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "    status.value = \"âœ… Text extracted from PDF.\"\n",
    "    return text_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d311320-0a6c-4ae6-ab5b-e8e43f89c1fa",
   "metadata": {},
   "source": [
    "#### Function to split the text file into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63859c8-6529-4906-8aa6-6afb894718be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(text_file_path, status):\n",
    "    status.value = \"Splitting text into chunks...\"\n",
    "    with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        full_text = file.read()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=700,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    status.value = f\"âœ… Chunks created: {len(chunks)}\"\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c079e6a-9def-484d-90a6-c1413e5195b0",
   "metadata": {},
   "source": [
    "#### Function for embedding text chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b14044c8-4451-41f5-aad4-68adb9940ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_chunks(chunks, status, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    status.value = \"Embedding chunks...\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    status.value = f\"âœ… Embeddings generated: {len(embeddings)}\"\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ecc55-49fa-4af0-8c84-1b7b28549797",
   "metadata": {},
   "source": [
    "#### Function to store the embeddings in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02018866-b515-48c0-aa04-f38ce83c94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_faiss(embeddings, chunks, status, index_file=\"faiss_index.index\"):\n",
    "    status.value = \"Saving embeddings to FAISS index...\"\n",
    "    embeddings = np.array(embeddings).astype(\"float32\")\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_file)\n",
    "    status.value = f\"âœ… FAISS index saved as {index_file}\"\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebfd3d-18b7-4534-81b2-2b02884605a2",
   "metadata": {},
   "source": [
    "#### Function to send the user query along with retrieved chunks to the LLM for response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164dd23b-45a3-405d-9ec9-ad0b8564fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm_with_context(query, relevant_chunks, status):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    status.value = \"ðŸ¤– Asking the LLM...\"\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use only the information provided in the context below to answer the question. \n",
    "If the context does not contain information relevant to the question, respond with:\n",
    "\"There is no content related to your query in the provided document.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    response = client.chat(model=\"llama3.2\", messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])\n",
    "    \n",
    "    status.value = \"âœ… Response generated!\"\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56319c9-a1e2-44be-8868-c92b2de6ce95",
   "metadata": {},
   "source": [
    "#### Basic UI for demonstration purposes using widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b34f804f-139d-472f-ab0c-238ca86a16c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6edb84cbe94bb580a4144a17032f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<div style='margin-bottom: 20px;'><h1 style='font-size: 36px;'>PageWhisper</h1><p sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heading = widgets.HTML(\n",
    "    \"<div style='margin-bottom: 20px;'>\"\n",
    "    \"<h1 style='font-size: 36px;'>PageWhisper</h1>\"\n",
    "    \"<p style='font-size: 18px; color: #555;'>Ask context-aware questions from your uploaded PDFs using RAG + LLM magic</p>\"\n",
    "    \"</div>\"\n",
    ")\n",
    "\n",
    "\n",
    "spacer = widgets.Box(layout=widgets.Layout(height='30px'))\n",
    "file_name_label = widgets.Label(value=\"ðŸ“„ No file uploaded yet.\")\n",
    "pdf_upload = widgets.FileUpload(accept='.pdf', multiple=False)\n",
    "query_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Ask your question...',\n",
    "    description='Query:',\n",
    "    layout=widgets.Layout(width='auto', height='100px')\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run PDF QA\", button_style='success')\n",
    "status_widget = widgets.Label(value=\"\")\n",
    "output = widgets.Output()\n",
    "\n",
    "query_input.layout.display = 'none'\n",
    "run_button.layout.display = 'none'\n",
    "\n",
    "chunks = []\n",
    "index = None\n",
    "pdf_name = \"\"\n",
    "\n",
    "display(widgets.VBox([\n",
    "    heading,\n",
    "    file_name_label,\n",
    "    pdf_upload,\n",
    "    status_widget,\n",
    "    query_input,\n",
    "    spacer,\n",
    "    run_button,\n",
    "    output\n",
    "]))\n",
    "\n",
    "\n",
    "\n",
    "def on_pdf_upload_change(change):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        status_widget.value = \"ðŸ”„ Processing PDF...\"\n",
    "\n",
    "        if len(pdf_upload.value) > 0:\n",
    "            uploaded_file = pdf_upload.value[0] \n",
    "            pdf_name = uploaded_file['name']\n",
    "            pdf_bytes = uploaded_file['content']\n",
    "            file_name_label.value = f\"ðŸ“„ Uploaded File: {pdf_name}\"\n",
    "\n",
    "\n",
    "            with open(pdf_name, 'wb') as f:\n",
    "                f.write(pdf_bytes)\n",
    "\n",
    " \n",
    "            global chunks, index\n",
    "            text_file = extract_pdf(pdf_name, status_widget)\n",
    "            chunks = chunking(text_file, status_widget)\n",
    "            embeddings = embed_chunks(chunks, status_widget)\n",
    "            index = save_embeddings_faiss(embeddings, chunks, status_widget)\n",
    "\n",
    "            status_widget.value = \"âœ… PDF processed! You can now ask your question.\"\n",
    "\n",
    "\n",
    "            query_input.layout.display = 'block'\n",
    "            run_button.layout.display = 'block'\n",
    "\n",
    "def on_run_button_click(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        status_widget.value = \"\"\n",
    "\n",
    "        if not index or not chunks:\n",
    "            print(\"Please upload and process a PDF first.\")\n",
    "            return\n",
    "\n",
    "        if query_input.value.strip():\n",
    "            query = query_input.value.strip()\n",
    "            status_widget.value = \"Searching for relevant chunks...\"\n",
    "\n",
    "\n",
    "            embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "            query_embedding = embed_model.encode([query])\n",
    "            D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "            retrieved_chunks = [chunks[i] for i in I[0]]\n",
    "\n",
    "            response = ask_llm_with_context(query, retrieved_chunks, status_widget)\n",
    "            print(\"ðŸ˜Ž Answer for your Query : \\n\",response)\n",
    "        else:\n",
    "            print(\"Enter a query to proceed.\")\n",
    "\n",
    "# Event Bindings\n",
    "pdf_upload.observe(on_pdf_upload_change, names='value')\n",
    "run_button.on_click(on_run_button_click)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
